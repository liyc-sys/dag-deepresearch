# exp4_report_drb: DRBè¯„ä¼°å¯¹é½ + Reportæ¡†æ¶å…¨é¢ä¼˜åŒ–

## å®éªŒç›®æ ‡

å°†exp3çš„DRBè¯„ä¼°ä»è‡ªå®šä¹‰5ç»´åº¦è¯„åˆ†å¯¹é½åˆ°å®˜æ–¹RACEè¯„ä¼°ï¼ˆreference-based 4ç»´åº¦ï¼‰ï¼Œå¹¶é€šè¿‡promptå’Œæ¶æ„ä¼˜åŒ–æå‡Reportè´¨é‡ã€‚

**æ ¸å¿ƒé—®é¢˜**: exp3åœ¨DRBè¯„ä¼°ä¸Šä½¿ç”¨äº†è‡ªå®šä¹‰5ç»´åº¦è¯„åˆ†ï¼ˆ1-5åˆ†åˆ¶ï¼‰ï¼Œä¸å®˜æ–¹DRBè¯„ä¼°ï¼ˆRACE 4ç»´åº¦ reference-basedï¼‰å®Œå…¨ä¸åŒï¼Œå¯¼è‡´ç»“æœæ— æ³•ä¸leaderboardå¯¹æ¯”ã€‚

**è§£å†³æ–¹æ¡ˆ**:
1. å½»åº•å¯¹é½å®˜æ–¹RACEè¯„ä¼°ï¼š`score = target/(target+ref)`ï¼Œ>0.5è¡¨ç¤ºä¼˜äºreference
2. å…¨é¢ä¼˜åŒ–Reportæ¡†æ¶ï¼šä»Promptã€æ¶æ„å‚æ•°ã€æœç´¢æ·±åº¦ç­‰å¤šç»´åº¦ä¼˜åŒ–

## ç›®å½•ç»“æ„

```
exp4_report_drb/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ task1.md                      # åŸå§‹ä»»åŠ¡è®¡åˆ’
â”‚   â””â”€â”€ README.md                     # æœ¬æ–‡æ¡£
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ input/                        # ç¬¦å·é“¾æ¥åˆ°exp3æ•°æ®
â”‚   â”‚   â”œâ”€â”€ drb_med_med.jsonl         # DRBé—®é¢˜é›†(50æ¡)
â”‚   â”‚   â”œâ”€â”€ exp3_report_drb_med_med.jsonl  # exp3çš„Reportç»“æœ
â”‚   â”‚   â”œâ”€â”€ reference.jsonl           # å®˜æ–¹reference articles(100æ¡,å…¶ä¸­50æ¡åŒ¹é…)
â”‚   â”‚   â””â”€â”€ criteria.jsonl            # å®˜æ–¹è¯„ä¼°æ ‡å‡†(100æ¡,å…¶ä¸­50æ¡åŒ¹é…)
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”œâ”€â”€ report_v2_drb_med.jsonl   # v2ä¼˜åŒ–åçš„Reportç»“æœ(ğŸ”„ 10/50å®Œæˆ)
â”‚   â”‚   â”œâ”€â”€ comparison_report.html    # (step4äº§å‡º) å¯¹æ¯”å¯è§†åŒ–
â”‚   â”‚   â””â”€â”€ scored/
â”‚   â”‚       â”œâ”€â”€ baseline_race_scored.jsonl      # âœ… baseline RACEè¯„åˆ†(50æ¡)
â”‚   â”‚       â”œâ”€â”€ baseline_race_scored_summary.json
â”‚   â”‚       â”œâ”€â”€ v2_race_scored.jsonl            # â³ v2 RACEè¯„åˆ†(å¾…step2å…¨é‡å®Œæˆ)
â”‚   â”‚       â”œâ”€â”€ v2_test_race_scored.jsonl       # âœ… v2 RACEè¯„åˆ†(2æ¡æµ‹è¯•)
â”‚   â”‚       â””â”€â”€ v2_test_race_scored_summary.json
â”‚   â””â”€â”€ logs/
â”‚       â”œâ”€â”€ step1_baseline_race.log
â”‚       â”œâ”€â”€ step2_test.log
â”‚       â”œâ”€â”€ step2_full_run.log        # ğŸ”„ å…¨é‡æ¨ç†æ—¥å¿—(æŒç»­å†™å…¥)
â”‚       â””â”€â”€ step3_v2_test.log
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ report_prompts_v2.yaml        # ä¼˜åŒ–åçš„Report prompté…ç½®
â”œâ”€â”€ step1_baseline_race.py            # baseline RACEè¯„ä¼°
â”œâ”€â”€ step2_optimize_prompts.py         # ä¼˜åŒ–æ¨ç†è„šæœ¬
â”œâ”€â”€ step3_race_eval.py                # é€šç”¨RACEè¯„ä¼°è„šæœ¬
â””â”€â”€ step4_compare.py                  # å¯¹æ¯”åˆ†æ+å¯è§†åŒ–
```

## æ‰§è¡ŒçŠ¶å†µ

### Step 1: Baseline RACEè¯„ä¼° âœ… å®Œæˆ

ç”¨exp3çš„Reportç»“æœï¼ˆ50æ¡ï¼‰è·‘å®˜æ–¹RACEè¯„ä¼°:
- Judgeæ¨¡å‹: è±†åŒ… Seed 1.6 (`ep-20250724221742-fddgp`)
- è¯„åˆ†æ–¹å¼: reference-based comparative scoring
- å½’ä¸€åŒ–: `score = target / (target + reference)`, >0.5è¡¨ç¤ºä¼˜äºreference

**Baselineç»“æœ:**
| ç»´åº¦ | å…¨é‡(50) | ä¸­æ–‡(25) | è‹±æ–‡(25) |
|------|----------|----------|----------|
| Comprehensiveness | 0.4276 | 0.4331 | 0.4222 |
| Insight | 0.3989 | 0.3964 | 0.4014 |
| Instruction Following | 0.4681 | 0.4691 | 0.4671 |
| Readability | 0.4557 | 0.4590 | 0.4523 |
| **Overall Score** | **0.4316** | **0.4334** | **0.4298** |

ç»“è®º: å…¨éƒ¨ä½äº0.5ï¼Œè¯´æ˜exp3çš„Reportæ•´ä½“å¼±äºreference articleã€‚Insightæ˜¯æœ€è–„å¼±ç»´åº¦(0.3989)ã€‚

### Step 2: ä¼˜åŒ–æ¨ç† ğŸ”„ è¿›è¡Œä¸­ï¼ˆ10/50å·²å®Œæˆï¼‰

**ä¼˜åŒ–ç­–ç•¥ï¼ˆä¸‰é˜¶æ®µï¼‰:**
1. **Planningé˜¶æ®µ**: è¦æ±‚æå–æ‰€æœ‰æ˜¾å¼/éšå¼éœ€æ±‚ï¼Œsectionæ•°6-10ï¼Œæ¯ä¸ªsectionæœ‰ç²¾ç¡®searchable query
2. **Researché˜¶æ®µ**: å¤šè§’åº¦æœç´¢(â‰¥3)ï¼Œæƒå¨æ¥æºä¼˜å…ˆ(PubMed/WHO)ï¼Œæ¯ä¸ªclaimå¿…é¡»æœ‰å¼•ç”¨URL
3. **Synthesisé˜¶æ®µ**: å­¦æœ¯çº§å†™ä½œï¼Œå¹³æ»‘è¿‡æ¸¡ï¼Œäº¤å‰å¼•ç”¨ï¼Œæ¯”è¾ƒè¡¨æ ¼ï¼Œ3000-6000å­—

**æ¶æ„å‚æ•°ä¼˜åŒ–:**
| å‚æ•° | exp3é»˜è®¤ | v2ä¼˜åŒ– | è¯´æ˜ |
|------|---------|--------|------|
| max_section_steps | 20 | 30 | æ›´æ·±å…¥æœç´¢ |
| summary_interval | 6 | 8 | æ›´å¤šæœç´¢åå†æ€»ç»“ |
| section_concurrency | 5 | 4 | å‡å°‘APIé™æµ |
| max_section_retries | 2 | 3 | æ›´ç¨³å¥ |
| prompts_type | default | medical | åŒ»å­¦ä¼˜åŒ–prompts |

**V2æŠ¥å‘Šè´¨é‡å¯¹æ¯”:**
| æŒ‡æ ‡ | exp3 baseline | v2 optimized (10æ¡) | å€æ•° |
|------|--------------|--------------|------|
| å¹³å‡æŠ¥å‘Šé•¿åº¦(å­—ç¬¦) | 10,081 | 44,600 | 4.4x |
| sectionæ•°é‡ | 4-6 | 10 | ~2x |
| å¼•ç”¨æ•°é‡/ç¯‡ | ~10 | 60+ | ~6x |
| å¹³å‡ç”Ÿæˆæ—¶é—´(s) | ~300 | ~3,044 | ~10x |

**å·²å®Œæˆ10æ¡è¯¦æƒ…:**
| task_id | æŠ¥å‘Šé•¿åº¦ | è€—æ—¶(s) | é—®é¢˜æ‘˜è¦ |
|---------|---------|---------|---------|
| 24 | 33,779 | 3,613 | å¦‚ä½•å¢å¼ºè‡ªé—­ç—‡å­¦ç”Ÿè¯¾å ‚å‚ä¸åº¦ |
| 25 | 20,958 | 3,384 | ä¸­æ€§ç²’ç»†èƒåœ¨è„‘ç¼ºè¡€æ€¥æ€§æœŸå’Œæ…¢æ€§æœŸçš„åŠŸèƒ½ |
| 26 | 58,821 | 3,365 | CD8+ Tç»†èƒçº¿ç²’ä½“åŠ¨åŠ›å­¦ |
| 27 | 30,204 | 3,037 | AIå¿ƒç†å’¨è¯¢å’Œäººç±»å¿ƒç†å’¨è¯¢æœ‰æœºç»“åˆ |
| 28 | 49,852 | 2,675 | è¯ç‰©ç ”ç©¶å¤šç»„å­¦è§’åº¦è§£æ |
| 42 | 49,687 | 1,885 | æ•™è‚²å¼ºå›½å­¦ç”Ÿä½“è´¨å¼ºå¥è®¡åˆ’ |
| 48 | 22,849 | 3,377 | äº”åä¸‰å²å¥åº·é£Ÿè°±è¥å…»æ­é… |
| 49 | 48,810 | 3,259 | 20-30å²å¥³æ€§å£è…”æ­£ç•¸å’ŒåŒ»ç¾éœ€æ±‚ |
| 50 | 65,910 | 3,143 | å­©å­èº«å¿ƒå¥åº·æˆé•¿å­¦ä¹ ç”Ÿæ´»å®‰æ’ |
| 75 | 65,131 | 2,706 | Therapeutic interventions plasma modulation |

**2æ¡v2 vs baseline RACEå¿«é€ŸéªŒè¯:**
| ç»´åº¦ | v2(2æ¡) | baseline(50æ¡avg) | æå‡ |
|------|---------|---------|------|
| Comprehensiveness | 0.4963 | 0.4276 | +0.0687 |
| Insight | 0.4687 | 0.3989 | +0.0698 |
| Instruction Following | 0.5036 | 0.4681 | +0.0355 |
| Readability | 0.4834 | 0.4557 | +0.0277 |
| **Overall Score** | **0.4864** | **0.4316** | **+0.0548** |

**å…¨é‡è¿è¡ŒçŠ¶æ€:** 40æ¡å‰©ä½™ï¼Œå¹¶å‘2ï¼Œåœ¨åå°æŒç»­è¿è¡Œï¼ˆè¿›ç¨‹PID 36665ï¼‰

### Step 3: v2 RACEè¯„ä¼° â³ å¾…step2å®Œæˆ

```bash
# step2å…¨é‡å®Œæˆåæ‰§è¡Œ:
cd /mnt/bn/med-mllm-lfv2/linjh/project/learn/idke/Agent-Factory-Med/others/dag-deepresearch/work/exp4_report_drb
python3 step3_race_eval.py --input assets/output/report_v2_drb_med.jsonl --tag v2 2>&1 | tee assets/logs/step3_v2_full.log
```

### Step 4: å¯¹æ¯”åˆ†æ+å¯è§†åŒ– â³ å¾…step3å®Œæˆ

```bash
# step3å®Œæˆåæ‰§è¡Œ:
python3 step4_compare.py
show assets/output/comparison_report.html exp4_drb_compare "exp4 DRB RACE: Baseline vs V2 comparison"
```

## ä»£ç ç»†èŠ‚

### step1_baseline_race.py
- é€šè¿‡`question`å­—æ®µåŒ¹é…exp3è¾“å‡º â†” reference/criteriaçš„`prompt`å­—æ®µ
- ä½¿ç”¨å®˜æ–¹`generate_merged_score_prompt`æ„å»ºè¯„åˆ†prompt
- article_1=æˆ‘ä»¬çš„report, article_2=reference article
- å¹¶å‘5çº¿ç¨‹è°ƒç”¨è±†åŒ…Seed 1.6 judge
- æ–­ç‚¹ç»­è·‘æ”¯æŒï¼ˆé€šè¿‡task_idæ£€æŸ¥å·²æœ‰ç»“æœï¼‰
- **å…³é”®**: `load_dotenv`å¿…é¡»åœ¨`import DRB`ä¹‹å‰ï¼Œå¦åˆ™JINA_API_KEYç¼ºå¤±ä¼šæŠ¥é”™

### step2_optimize_prompts.py
- åŠ è½½è‡ªå®šä¹‰`prompts/report_prompts_v2.yaml`è¦†ç›–é»˜è®¤promptï¼ˆ`orchestrator.prompts = custom_prompts`ï¼‰
- ç›´æ¥å°†questionä½œä¸ºtopicä¼ å…¥ReportOrchestratorï¼ˆDRBä¸éœ€è¦"Final Answer"åŒ…è£…ï¼‰
- æ¯æ¡åˆ›å»ºç‹¬ç«‹çš„OpenAIServerModelå’ŒReportOrchestratorå®ä¾‹ï¼ˆé¿å…çŠ¶æ€æ±¡æŸ“ï¼‰
- æ”¯æŒæ–­ç‚¹ç»­è·‘ï¼ˆé€šè¿‡questionå­—æ®µå»é‡ï¼‰å’Œå¹¶å‘æ§åˆ¶ï¼ˆ`--concurrency`å‚æ•°ï¼‰
- è¾“å‡ºæ ¼å¼ä¸exp3å®Œå…¨å…¼å®¹ï¼ˆquestion, report, task_idç­‰å­—æ®µï¼‰

### step3_race_eval.py
- é€šç”¨RACEè¯„ä¼°ï¼šæ”¯æŒä»»æ„report JSONLæ–‡ä»¶
- é€šè¿‡`--tag`å‚æ•°åŒºåˆ†ä¸åŒå®éªŒçš„è¾“å‡ºï¼ˆå¦‚`--tag v2`ç”Ÿæˆ`v2_race_scored.jsonl`ï¼‰
- ä¸step1é€»è¾‘ä¸€è‡´ä½†æ›´é€šç”¨ï¼Œæ”¯æŒ`--model`, `--workers`, `--limit`å‚æ•°
- æ–­ç‚¹ç»­è·‘ï¼šæ£€æŸ¥outputæ–‡ä»¶ä¸­å·²æœ‰çš„task_id

### step4_compare.py
- æŒ‰task_idåŒ¹é…baselineå’Œv2ç»“æœ
- è®¡ç®—å„ç»´åº¦deltaå’Œç™¾åˆ†æ¯”å˜åŒ–
- åˆ†è¯­è¨€(zh/en)ç»Ÿè®¡
- ç”Ÿæˆé™æ€HTMLå¯è§†åŒ–æŠ¥å‘Šï¼ˆsummary cards + bar charts + language breakdown + per-item table + top improvers/declinersï¼‰

### prompts/report_prompts_v2.yaml
- **report_planning**: å¢åŠ "CRITICAL: Instruction Analysis"ï¼Œè¦æ±‚æå–ALL explicit/implicit requirementsï¼Œæ¯ä¸ªrequirementæ˜ å°„åˆ°è‡³å°‘ä¸€ä¸ªsection
- **section_research**: å¢åŠ "Research Standards"ï¼Œ7æ¡æ ‡å‡†ï¼ˆbreadth, depth, recency, authority, citation, counterpoints, specificityï¼‰
- **report_synthesis**: å¢åŠ "Writing Standards"ï¼Œ8æ¡æ ‡å‡†ï¼ˆacademic tone, smooth transitions, executive summary, analytical depth, data presentation, citation format, language matching, lengthï¼‰

## ä¸å®˜æ–¹RACEè¯„ä¼°çš„å¯¹é½åˆ†æ

### å®Œå…¨å¯¹é½çš„éƒ¨åˆ†ï¼ˆç›´æ¥importå®˜æ–¹ä»£ç ï¼‰
- è¯„åˆ†Promptï¼ˆ`generate_merged_score_prompt` ä¸­/è‹±æ–‡ï¼‰
- åŠ æƒåˆ†æ•°è®¡ç®—ï¼ˆ`calculate_weighted_scores`ï¼‰
- JSONè§£æï¼ˆ`extract_json_from_markdown`ï¼‰
- å½’ä¸€åŒ–å…¬å¼ï¼š`target / (target + reference)`
- articleåˆ†é…ï¼šarticle_1=targetï¼ˆæˆ‘ä»¬çš„reportï¼‰ï¼Œarticle_2=referenceï¼ˆå®˜æ–¹å‚è€ƒæ–‡ç« ï¼‰
- criteria/referenceåŒ¹é…é€»è¾‘ï¼ˆé€šè¿‡promptå­—æ®µåŒ¹é…ï¼‰

### æœªå¯¹é½çš„å·®å¼‚
| å·®å¼‚é¡¹ | å®˜æ–¹ | exp4 | å½±å“ç¨‹åº¦ |
|--------|------|------|---------|
| Judgeæ¨¡å‹ | Gemini 2.5 Pro + thinking_budget=16000 | è±†åŒ…Seed 1.6, temp=0.1, max_tokens=8192 | é«˜ï¼ˆç»å¯¹åˆ†æ•°ä¸å¯ç›´æ¥æ¯”è¾ƒleaderboardï¼‰ |
| Article Cleaning | è¯„åˆ†å‰ç”¨ArticleCleaneræ¸…æ´— | æ— ï¼Œç›´æ¥ç”¨åŸå§‹report | ä¸­ |
| è¯­è¨€æ£€æµ‹ | ä»æ•°æ®è¯»å–languageæ ‡ç­¾ | å¯å‘å¼æ£€æµ‹ï¼ˆä¸­æ–‡å­—ç¬¦å æ¯”>20%ï¼‰ | ä½ |

**ç»“è®º**: ç®—æ³•å±‚é¢å®Œå…¨å¯¹é½ï¼Œç”¨åŒä¸€ä¸ªjudgeåšbaseline vs v2çš„ç›¸å¯¹å¯¹æ¯”å®Œå…¨åˆç†ã€‚ç»å¯¹åˆ†æ•°å› judgeæ¨¡å‹ä¸åŒï¼Œä¸å¯ç›´æ¥ä¸å®˜æ–¹leaderboardæ¯”è¾ƒã€‚

## ä¾èµ–å…³ç³»

| ä¾èµ– | è·¯å¾„ |
|------|------|
| FlashOAgents (Reportæ¡†æ¶) | `../../FlashOAgents/` |
| DRBå®˜æ–¹ä»“åº“ | `../exp3_med_full/official_repos/deep_research_bench/` |
| APIé…ç½®(.env) | `../../../../0001_utils/api/.env` |
| è±†åŒ…Seed 1.6 | endpoint: `ep-20250724221742-fddgp` |

## åç»­è®¡åˆ’

1. **ç­‰step2å…¨é‡å®Œæˆ** â†’ æ£€æŸ¥è¿›åº¦: `wc -l assets/output/report_v2_drb_med.jsonl`
2. **è·‘step3 RACEè¯„ä¼°** â†’ å‘½ä»¤è§ä¸Šæ–¹Step 3
3. **è·‘step4å¯¹æ¯”åˆ†æ** â†’ å‘½ä»¤è§ä¸Šæ–¹Step 4ï¼Œç”¨showéƒ¨ç½²HTML
4. **æ ¹æ®ç»“æœè¿­ä»£ä¼˜åŒ–**:
   - å¦‚æœInsightä»ä½ â†’ åŠ å¼ºsynthesis promptä¸­çš„åˆ†ææ·±åº¦è¦æ±‚
   - å¦‚æœæŸäº›é¢˜ç›®ä¸‹é™ â†’ åˆ†æåŸå› ï¼Œé’ˆå¯¹æ€§è°ƒæ•´
   - å¦‚æœå¼•ç”¨ç‡ä½ â†’ åŠ å¼ºresearché˜¶æ®µçš„URLä¿ç•™è¦æ±‚
   - å¦‚æœComprehensivenessä¸å¤Ÿ â†’ å¢åŠ sectionæ•°é‡ä¸Šé™
