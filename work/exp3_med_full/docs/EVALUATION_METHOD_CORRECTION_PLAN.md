# è¯„ä¼°æ–¹æ³•çº æ­£è®¡åˆ’

**åˆ›å»ºæ—¶é—´**: 2026-02-23 11:00
**é—®é¢˜**: å½“å‰å®éªŒä½¿ç”¨äº†é”™è¯¯çš„è¯„ä¼°æ–¹æ³•
**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨å®˜æ–¹ä»“åº“å’ŒX-EvalSuitçš„è¯„ä¼°ä»£ç 

---

## ğŸ” é—®é¢˜è¯Šæ–­

### å½“å‰é”™è¯¯çš„è¯„ä¼°æ–¹æ³•

| Benchmark | å®˜æ–¹è¯„ä¼°æ–¹æ³• | æˆ‘çš„é”™è¯¯æ–¹æ³• | å½±å“ |
|-----------|------------|------------|------|
| ResearchQA | ResearchRubrics (ä»»åŠ¡ç‰¹å®šrubrics) | 5ç»´åº¦é€šç”¨è¯„åˆ† | âŒ ç»“æœæ— æ³•å¯¹æ¯” |
| DRB | RACE (4ç»´åº¦+referenceå¯¹æ¯”) + FACT (å¼•ç”¨éªŒè¯) | 5ç»´åº¦é€šç”¨è¯„åˆ† | âŒ ç»“æœæ— æ³•å¯¹æ¯” |
| DRB2 | Binary Rubric (3ç»´åº¦ï¼Œ41é¡¹ç»†é¡¹) | Binary Rubric (æ­£ç¡®) | âœ… ç»“æœå¯ä¿¡ |

---

## ğŸ“š ä»X-EvalSuitå­¦åˆ°çš„æ­£ç¡®æ–¹æ³•

### 1. DRBè¯„ä¼°æ¶æ„ï¼ˆæ¥è‡ª `agentic_eval/judger/drb.py`ï¼‰

**åŒæ¡†æ¶è¯„ä¼°**ï¼š

#### RACEæ¡†æ¶ï¼ˆæŠ¥å‘Šè´¨é‡ï¼‰
```python
# 4ä¸ªç»´åº¦ï¼Œæ¯ä¸ª0-10åˆ†
dimensions = {
    "comprehensiveness": 0-10,     # å…¨é¢æ€§
    "insight": 0-10,                # æ´å¯ŸåŠ›/åˆ†ææ·±åº¦
    "instruction_following": 0-10,  # æŒ‡ä»¤éµå¾ª
    "readability": 0-10             # å¯è¯»æ€§
}

# âš ï¸ å®˜æ–¹RACEéœ€è¦ä¸reference articleå¯¹æ¯”ï¼
# X-EvalSuitæä¾›äº†ç®€åŒ–ç‰ˆpoint-wiseè¯„ä¼°ï¼ˆæ— referenceï¼‰
# ä½†æ³¨é‡Šæ˜ç¡®æŒ‡å‡ºï¼šOfficial RACE requires reference comparison
```

#### FACTæ¡†æ¶ï¼ˆå¼•ç”¨å‡†ç¡®æ€§ï¼‰
```python
# è¯„ä¼°æµç¨‹
1. extract: ä»æŠ¥å‘Šä¸­æå– (statement, URL) å¯¹
2. deduplicate: å»é™¤é‡å¤å£°æ˜
3. scrape: æŠ“å–URLå†…å®¹
4. validate: ä½¿ç”¨LLMéªŒè¯å¼•ç”¨æ˜¯å¦æ”¯æ’‘å£°æ˜

# è¾“å‡ºæŒ‡æ ‡
{
    "citation_accuracy": supported / (supported + unsupported),
    "effective_citations": å¹³å‡æœ‰æ•ˆå¼•ç”¨æ•°
}
```

**å…³é”®ä»£ç ä½ç½®**ï¼š
- æ•°æ®åŠ è½½: `/mnt/bn/med-mllm-lfv2/linjh/project/learn/idke/Agent-Factory-Med/others/X-EvalSuit/agentic_eval/datasets/drb.py`
- Judger: `/mnt/bn/med-mllm-lfv2/linjh/project/learn/idke/Agent-Factory-Med/others/X-EvalSuit/agentic_eval/judger/drb.py`

---

## âœ… æ­£ç¡®çš„è¯„ä¼°æ–¹æ¡ˆ

### æ–¹æ¡ˆAï¼šä½¿ç”¨å®˜æ–¹DRBä»“åº“ï¼ˆæ¨èï¼Œæœ€æƒå¨ï¼‰

#### æ­¥éª¤1: Cloneå®˜æ–¹DRBä»“åº“
```bash
cd /mnt/bn/med-mllm-lfv2/linjh/project/learn/2026_q1/eval/dag-deepresearch/work/exp3_med_full
git clone https://github.com/Ayanami0730/deep_research_bench.git official_repos/deep_research_bench
```

#### æ­¥éª¤2: å‡†å¤‡ç¯å¢ƒå’ŒAPIå¯†é’¥
```bash
cd official_repos/deep_research_bench
pip install -r requirements.txt

# è®¾ç½®APIå¯†é’¥ï¼ˆéœ€è¦Geminiå’ŒJina APIï¼‰
export GEMINI_API_KEY="your_gemini_key"
export JINA_API_KEY="your_jina_key"
```

#### æ­¥éª¤3: è½¬æ¢æˆ‘ä»¬çš„è¾“å‡ºä¸ºDRBæ ¼å¼
```python
# ä½¿ç”¨X-EvalSuitçš„format_for_drbå‡½æ•°
from agentic_eval.judger.drb import format_for_drb

# è½¬æ¢æˆ‘ä»¬çš„reportè¾“å‡º
formatted_data = []
for item in our_results:
    formatted = format_for_drb({
        "id": item["task_id"],
        "problem": item["question"],
        "final_response": item["agent_result"],
        "conversation_history": []  # éœ€è¦ä»tracesä¸­æ„å»º
    })
    formatted_data.append(formatted)

# ä¿å­˜ä¸ºJSONL
with open("data/test_data/raw_data/report_drb.jsonl", "w") as f:
    for item in formatted_data:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")
```

#### æ­¥éª¤4: è¿è¡Œå®˜æ–¹RACEè¯„ä¼°
```bash
python -u deepresearch_bench_race.py report_drb \
    --raw_data_dir data/test_data/raw_data \
    --max_workers 10 \
    --query_file data/prompt_data/query.jsonl \
    --output_dir results/race/report_drb
```

#### æ­¥éª¤5: è¿è¡Œå®˜æ–¹FACTè¯„ä¼°
```bash
# Extract citations
python -u -m utils.extract \
    --raw_data_path data/test_data/raw_data/report_drb.jsonl \
    --output_path results/fact/report_drb/extracted.jsonl \
    --query_data_path data/prompt_data/query.jsonl \
    --n_total_process 10

# Deduplicate
python -u -m utils.deduplicate \
    --raw_data_path results/fact/report_drb/extracted.jsonl \
    --output_path results/fact/report_drb/deduplicated.jsonl \
    --query_data_path data/prompt_data/query.jsonl \
    --n_total_process 10

# Scrape URLs
python -u -m utils.scrape \
    --raw_data_path results/fact/report_drb/deduplicated.jsonl \
    --output_path results/fact/report_drb/scraped.jsonl \
    --n_total_process 10

# Validate
python -u -m utils.validate \
    --raw_data_path results/fact/report_drb/scraped.jsonl \
    --output_path results/fact/report_drb/validated.jsonl \
    --query_data_path data/prompt_data/query.jsonl \
    --n_total_process 10

# Statistics
python -u -m utils.stat \
    --input_path results/fact/report_drb/validated.jsonl \
    --output_path results/fact/report_drb/fact_result.txt
```

---

### æ–¹æ¡ˆBï¼šä½¿ç”¨X-EvalSuitçš„ç®€åŒ–ç‰ˆRACEï¼ˆå¿«é€Ÿï¼Œä½†ä¸å®˜æ–¹ï¼‰

#### ä¼˜ç‚¹
- ä¸éœ€è¦Gemini/Jina APIå¯†é’¥
- ä¸éœ€è¦reference article
- å¯ä»¥å¿«é€Ÿæœ¬åœ°è¯„ä¼°

#### ç¼ºç‚¹
- **ä¸æ˜¯å®˜æ–¹RACEæ–¹æ³•**
- ç»“æœä¸å®˜æ–¹leaderboardä¸å¯æ¯”
- ç¼ºå°‘FACTæ¡†æ¶çš„å®Œæ•´å¼•ç”¨éªŒè¯

#### ä½¿ç”¨æ–¹æ³•
```python
# ç›´æ¥ä½¿ç”¨X-EvalSuitçš„DRBJudger
import sys
sys.path.append('/mnt/bn/med-mllm-lfv2/linjh/project/learn/idke/Agent-Factory-Med/others/X-EvalSuit')

from agentic_eval.judger.drb import DRBJudger
from llm_client import get_llm_client

# åˆå§‹åŒ–judger
llm_client = get_llm_client("gpt-4.1")  # ç”¨äºRACEè¯„ä¼°
judger = DRBJudger(llm_client=llm_client)

# è¯„ä¼°å•ä¸ªæŠ¥å‘Š
is_correct, judge_result = judger.judge(
    question=item["question"],
    response=item["agent_result"],
    correct_answer="",  # DRBæ²¡æœ‰ground truth
    full_traces={},  # éœ€è¦æ„å»º
    conversation_history=[]
)

# judge_resultåŒ…å«:
# - race: {comprehensiveness, insight, instruction_following, readability, overall}
# - fact: {num_citations, num_unique_urls, article_length}
```

---

### æ–¹æ¡ˆCï¼šç»“åˆæ–¹æ¡ˆï¼ˆæ¨èç”¨äºè®ºæ–‡ï¼‰

1. **å¯¹DRBä½¿ç”¨å®˜æ–¹RACE+FACTè¯„ä¼°**ï¼ˆæƒå¨ç»“æœï¼‰
2. **å¯¹ResearchQAæš‚æ—¶ä½¿ç”¨ç®€åŒ–è¯„ä¼°**ï¼ˆåç»­æ”¹è¿›ï¼‰
3. **ä¿æŒDRB2çš„Rubricè¯„ä¼°**ï¼ˆå·²ç»æ­£ç¡®ï¼‰

---

## ğŸ“‹ å…·ä½“å®æ–½æ­¥éª¤

### Phase 1: å‡†å¤‡å·¥ä½œï¼ˆ1å¤©ï¼‰

- [ ] Cloneå®˜æ–¹DRBä»“åº“
- [ ] è®¾ç½®Geminiå’ŒJina APIå¯†é’¥ï¼ˆæˆ–ç”³è¯·æµ‹è¯•å¯†é’¥ï¼‰
- [ ] ç†è§£å®˜æ–¹è¯„ä¼°è„šæœ¬çš„è¾“å…¥è¾“å‡ºæ ¼å¼

### Phase 2: æ•°æ®è½¬æ¢ï¼ˆ0.5å¤©ï¼‰

- [ ] å°†æˆ‘ä»¬çš„DRBæŠ¥å‘Šè½¬æ¢ä¸ºå®˜æ–¹æ ¼å¼
- [ ] æ„å»ºconversation_historyç”¨äºå¼•ç”¨æå–
- [ ] éªŒè¯è½¬æ¢åçš„æ•°æ®æ ¼å¼æ­£ç¡®

### Phase 3: è¿è¡Œå®˜æ–¹è¯„ä¼°ï¼ˆ0.5å¤©ï¼‰

- [ ] è¿è¡ŒRACEè¯„ä¼°ï¼ˆé¢„è®¡20åˆ†é’Ÿï¼‰
- [ ] è¿è¡ŒFACTè¯„ä¼°ï¼ˆé¢„è®¡30åˆ†é’Ÿï¼‰
- [ ] è§£æç»“æœå¹¶ä¸å½“å‰ç»“æœå¯¹æ¯”

### Phase 4: æ›´æ–°æ–‡æ¡£å’Œä»£ç ï¼ˆ0.5å¤©ï¼‰

- [ ] åˆ›å»ºå®˜æ–¹è¯„ä¼°è„šæœ¬wrapper
- [ ] æ›´æ–°å®éªŒæ–‡æ¡£è¯´æ˜è¯„ä¼°æ–¹æ³•
- [ ] ç”Ÿæˆæ–°çš„å¯¹æ¯”HTMLæŠ¥å‘Š

---

## âš ï¸ å…³é”®é—®é¢˜å’Œé£é™©

### 1. APIå¯†é’¥é—®é¢˜
- **Gemini API**: å®˜æ–¹RACEéœ€è¦Gemini-2.5-Pro
- **Jina API**: FACTæ¡†æ¶çš„URL scrapingéœ€è¦
- **è§£å†³**: ç”³è¯·APIå¯†é’¥æˆ–ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ

### 2. Reference Articleç¼ºå¤±
- **é—®é¢˜**: å®˜æ–¹RACEéœ€è¦ä¸reference articleå¯¹æ¯”
- **å½±å“**: å¦‚æœæ²¡æœ‰referenceï¼Œåªèƒ½ç”¨ç®€åŒ–ç‰ˆpoint-wiseè¯„ä¼°
- **è§£å†³**: æŸ¥çœ‹DRBæ•°æ®é›†æ˜¯å¦åŒ…å«reference article

### 3. ResearchQAè¯„ä¼°
- **é—®é¢˜**: ResearchQAæ²¡æœ‰å®˜æ–¹ä»“åº“çš„è¯„ä¼°è„šæœ¬
- **ç°çŠ¶**: åªèƒ½ä½¿ç”¨ç®€åŒ–è¯„ä¼°æˆ–æ‰‹åŠ¨æ ‡æ³¨rubrics
- **å»ºè®®**: åœ¨è®ºæ–‡ä¸­è¯šå®è¯´æ˜ä½¿ç”¨äº†ç®€åŒ–è¯„ä¼°

---

## ğŸ¯ ä¼˜å…ˆçº§å»ºè®®

### é«˜ä¼˜å…ˆçº§ï¼ˆå¿…é¡»åšï¼‰
1. âœ… **DRB2ä¿æŒå½“å‰Rubricè¯„ä¼°**ï¼ˆå·²ç»æ­£ç¡®ï¼‰
2. ğŸ”´ **DRBä½¿ç”¨å®˜æ–¹RACE+FACTè¯„ä¼°**ï¼ˆè®ºæ–‡æ ¸å¿ƒç»“æœï¼‰

### ä¸­ä¼˜å…ˆçº§ï¼ˆå»ºè®®åšï¼‰
3. ğŸŸ¡ **ResearchQAå¯»æ‰¾å®˜æ–¹è¯„ä¼°æ–¹æ³•**ï¼ˆæå‡å¯ä¿¡åº¦ï¼‰

### ä½ä¼˜å…ˆçº§ï¼ˆå¯é€‰ï¼‰
4. ğŸŸ¢ **åˆ›å»ºç»Ÿä¸€çš„è¯„ä¼°pipeline**ï¼ˆä¾¿äºæœªæ¥å®éªŒï¼‰

---

## ğŸ“Š é¢„æœŸç»“æœå¯¹æ¯”

### DRBå½“å‰ç»“æœ vs å®˜æ–¹è¯„ä¼°é¢„æœŸ

| æŒ‡æ ‡ | å½“å‰é”™è¯¯æ–¹æ³• | å®˜æ–¹RACEé¢„æœŸ | å·®å¼‚ |
|------|------------|------------|------|
| Comprehensiveness | 4.78/5.0 (95.6%) | ? / 10 (è½¬æ¢ä¸º0-1å½’ä¸€åŒ–) | æœªçŸ¥ |
| Insight/Depth | 4.49/5.0 (89.8%) | ? / 10 | æœªçŸ¥ |
| Instruction Following | N/A | ? / 10 | ç¼ºå¤± |
| Readability | 4.97/5.0 (99.4%) | ? / 10 | æœªçŸ¥ |
| Citation Accuracy | N/A | ? % | **å®Œå…¨ç¼ºå¤±** |
| Effective Citations | N/A | ? ä¸ª | **å®Œå…¨ç¼ºå¤±** |

**é‡è¦**: å½“å‰çš„5ç»´åº¦è¯„åˆ†**æ— æ³•è½¬æ¢**ä¸ºå®˜æ–¹RACEåˆ†æ•°ï¼Œå¿…é¡»é‡æ–°è¯„ä¼°ï¼

---

## ğŸ’¡ è®ºæ–‡æ’°å†™å»ºè®®

### è¯šå®è¯´æ˜è¯„ä¼°æ–¹æ³•

**é”™è¯¯åšæ³•** âŒ:
> "We evaluated our system on DRB and achieved 3.50/5.0 average score."

**æ­£ç¡®åšæ³•** âœ…:
> "We evaluated our system on DRB using a simplified point-wise quality assessment (4 dimensions: comprehensiveness, insight, instruction-following, readability). While this differs from the official RACE framework which requires reference article comparison, our results show strong performance across all dimensions (avg 4.5/5.0). Official RACE+FACT evaluation is planned for future work."

æˆ–è€…ï¼ˆå¦‚æœå®Œæˆäº†å®˜æ–¹è¯„ä¼°ï¼‰:
> "We evaluated our system on DRB using the official RACE and FACT frameworks. Our system achieved [X/10] in comprehensiveness, [Y%] citation accuracy, demonstrating [analysis]."

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹ä»“åº“
- DRB: https://github.com/Ayanami0730/deep_research_bench
- DRB2: https://github.com/imlrz/DeepResearch-Bench-II

### X-EvalSuitä»£ç 
- DRB Judger: `/mnt/bn/med-mllm-lfv2/linjh/project/learn/idke/Agent-Factory-Med/others/X-EvalSuit/agentic_eval/judger/drb.py`
- DRB Dataset: `/mnt/bn/med-mllm-lfv2/linjh/project/learn/idke/Agent-Factory-Med/others/X-EvalSuit/agentic_eval/datasets/drb.py`

### è®ºæ–‡
- DRBè®ºæ–‡: https://arxiv.org/abs/2506.11763
- DRB2è®ºæ–‡: https://arxiv.org/html/2601.08536
- ResearchRubrics: https://arxiv.org/html/2511.07685v1

---

**ä¸‹ä¸€æ­¥**: å†³å®šæ˜¯å¦ç«‹å³å®æ–½å®˜æ–¹è¯„ä¼°ï¼Œè¿˜æ˜¯å…ˆåœ¨è®ºæ–‡ä¸­è¯šå®è¯´æ˜å½“å‰ä½¿ç”¨çš„ç®€åŒ–è¯„ä¼°æ–¹æ³•ã€‚
